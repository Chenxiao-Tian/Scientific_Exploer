References:

Annexes/Appendices:
PublishedasaconferencepaperatICLR2017
2 FAST APPROXIMATE CONVOLUTIONS ON GRAPHS
Inthissection, weprovidetheoreticalmotivationforaspecificgraph-basedneuralnetworkmodel
f(X,A)thatwewilluseintherestofthispaper. Weconsideramulti-layerGraphConvolutional
Network(GCN)withthefollowinglayer-wisepropagationrule:
(cid:16) (cid:17)
H(l+1) =σ D˜−1 2A˜D˜− 21H(l)W(l) . (2)
Here,A˜ = A+I istheadjacencymatrixoftheundirectedgraphG withaddedself-connections.
N
I istheidentitymatrix,D˜ = (cid:80) A˜ andW(l) isalayer-specifictrainableweightmatrix. σ(·)
N ii j ij
denotesanactivationfunction,suchastheReLU(·)=max(0,·).H(l) ∈RN×Disthematrixofac-
tivationsinthelthlayer;H(0) =X. Inthefollowing,weshowthattheformofthispropagationrule
canbemotivated1 viaafirst-orderapproximationoflocalizedspectralfiltersongraphs(Hammond
etal.,2011;Defferrardetal.,2016).
2.1 SPECTRALGRAPHCONVOLUTIONS
We consider spectral convolutions on graphs defined as the multiplication of a signal x ∈ RN (a
scalarforeverynode)withafilterg = diag(θ)parameterizedbyθ ∈ RN intheFourierdomain,
θ
i.e.:
g (cid:63)x=Ug U(cid:62)x, (3)
θ θ
whereU isthematrixofeigenvectorsofthenormalizedgraphLaplacianL=I
N
−D− 21AD− 21 =
UΛU(cid:62), with a diagonal matrix of its eigenvalues Λ and U(cid:62)x being the graph Fourier transform
of x. We can understand g as a function of the eigenvalues of L, i.e. g (Λ). Evaluating Eq. 3 is
θ θ
computationallyexpensive,asmultiplicationwiththeeigenvectormatrixU isO(N2).Furthermore,
computingtheeigendecompositionofLinthefirstplacemightbeprohibitivelyexpensiveforlarge
graphs. Tocircumventthisproblem,itwassuggestedinHammondetal.(2011)thatg (Λ)canbe
θ
well-approximated by a truncated expansion in terms of Chebyshev polynomials T (x) up to Kth
k
order:
K
(cid:88)
g θ(cid:48)(Λ)≈ θ k(cid:48)T k(Λ˜), (4)
k=0
with a rescaled Λ˜ = 2 Λ−I . λ denotes the largest eigenvalue of L. θ(cid:48) ∈ RK is now a
λmax N max
vectorofChebyshevcoefficients. TheChebyshevpolynomialsarerecursivelydefinedasT (x) =
k
2xT (x)−T (x),withT (x) = 1andT (x) = x. ThereaderisreferredtoHammondetal.
k−1 k−2 0 1
(2011)foranin-depthdiscussionofthisapproximation.
Goingbacktoourdefinitionofaconvolutionofasignalxwithafilterg θ(cid:48),wenowhave:
K
(cid:88)
g θ(cid:48) (cid:63)x≈ θ k(cid:48)T k(L˜)x, (5)
k=0
withL˜ = 2 L−I ; ascaneasilybeverifiedbynoticingthat(UΛU(cid:62))k = UΛkU(cid:62). Notethat
λmax N
thisexpressionisnowK-localizedsinceitisaKth-orderpolynomialintheLaplacian,i.e.itdepends
onlyonnodesthatareatmaximumK stepsawayfromthecentralnode(Kth-orderneighborhood).
The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. Defferrard et al.
(2016)usethisK-localizedconvolutiontodefineaconvolutionalneuralnetworkongraphs.
2.2 LAYER-WISELINEARMODEL
A neural network model based on graph convolutions can therefore be built by stacking multiple
convolutionallayersoftheformofEq.5,eachlayerfollowedbyapoint-wisenon-linearity. Now,
imaginewelimitedthelayer-wiseconvolutionoperationtoK =1(seeEq.5),i.e.afunctionthatis
linearw.r.t.LandthereforealinearfunctiononthegraphLaplacianspectrum.
1WeprovideanalternativeinterpretationofthispropagationrulebasedontheWeisfeiler-Lehmanalgorithm
(Weisfeiler&Lehmann,1968)inAppendixA.
2
PublishedasaconferencepaperatICLR2017
Citationnetworks Weconsiderthreecitationnetworkdatasets: Citeseer,CoraandPubmed(Sen
etal.,2008). Thedatasetscontainsparsebag-of-wordsfeaturevectorsforeachdocumentandalist
ofcitationlinksbetweendocuments. Wetreatthecitationlinksas(undirected)edgesandconstruct
abinary,symmetricadjacencymatrixA. Eachdocumenthasaclasslabel. Fortraining,weonlyuse
20labelsperclass,butallfeaturevectors.
NELL NELLisadatasetextractedfromtheknowledgegraphintroducedin(Carlsonetal.,2010).
Aknowledgegraphisasetofentitiesconnectedwithdirected,labelededges(relations). Wefollow
the pre-processing scheme as described in Yang et al. (2016). We assign separate relation nodes
r and r for each entity pair (e ,r,e ) as (e ,r ) and (e ,r ). Entity nodes are described by
1 2 1 2 1 1 2 2
sparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot
representationforeveryrelationnode,effectivelyresultingina61,278-dimsparsefeaturevectorper
node. The semi-supervised task here considers the extreme case of only a single labeled example
perclassinthetrainingset. Weconstructabinary,symmetricadjacencymatrixfromthisgraphby
settingentriesA =1,ifoneormoreedgesarepresentbetweennodesiandj.
ij
Randomgraphs Wesimulaterandomgraphdatasetsofvarioussizesforexperimentswherewe
measuretrainingtimeperepoch. ForadatasetwithN nodeswecreatearandomgraphassigning
2N edgesuniformlyatrandom. WetaketheidentitymatrixI asinputfeaturematrixX,thereby
N
implicitlytakingafeaturelessapproachwherethemodelisonlyinformedabouttheidentityofeach
node,specifiedbyauniqueone-hotvector. WeadddummylabelsY =1foreverynode.
i
5.2 EXPERIMENTALSET-UP
Unless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate pre-
dictionaccuracyonatestsetof1,000labeledexamples. Weprovideadditionalexperimentsusing
deeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in Yang
et al. (2016) with an additional validation set of 500 labeled examples for hyperparameter opti-
mization(dropoutrateforalllayers,L2regularizationfactorforthefirstGCNlayerandnumberof
hiddenunits). Wedonotusethevalidationsetlabelsfortraining.
Forthecitationnetworkdatasets,weoptimizehyperparametersonCoraonlyandusethesameset
ofparametersforCiteseerandPubmed. Wetrainallmodelsforamaximumof200epochs(training
iterations)usingAdam(Kingma&Ba,2015)withalearningrateof0.01andearlystoppingwitha
windowsizeof10, i.e.westoptrainingifthevalidationlossdoesnotdecreasefor10consecutive
epochs. We initialize weights using the initialization described in Glorot & Bengio (2010) and
accordingly(row-)normalizeinputfeaturevectors. Ontherandomgraphdatasets,weuseahidden
layersizeof32unitsandomitregularization(i.e.neitherdropoutnorL2regularization).
5.3 BASELINES
We compare against the same baseline methods as in Yang et al. (2016), i.e. label propagation
(LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold
regularization(ManiReg)(Belkinetal.,2006)andskip-grambasedgraphembeddings(DeepWalk)
(Perozzietal.,2014). WeomitTSVM(Joachims,1999),asitdoesnotscaletothelargenumberof
classesinoneofourdatasets.
We further compare against the iterative classification algorithm (ICA) proposed in Lu & Getoor
(2003)inconjunctionwithtwologisticregressionclassifiers,oneforlocalnodefeaturesaloneand
one for relational classification using local features and an aggregation operator as described in
Sen et al. (2008). We first train the local classifier using all labeled training set nodes and use
it to bootstrap class labels of unlabeled nodes for relational classifier training. We run iterative
classification (relational classifier) with a random node ordering for 10 iterations on all unlabeled
nodes(bootstrappedusingthelocalclassifier).L2regularizationparameterandaggregationoperator
(countvs.prop,seeSenetal.(2008))arechosenbasedonvalidationsetperformanceforeachdataset
separately.
Lastly, we compare against Planetoid (Yang et al., 2016), where we always choose their best-
performingmodelvariant(transductivevs.inductive)asabaseline.
6

Body Text:
PublishedasaconferencepaperatICLR2017
SEMI-SUPERVISED CLASSIFICATION WITH
GRAPH CONVOLUTIONAL NETWORKS
ThomasN.Kipf MaxWelling
UniversityofAmsterdam UniversityofAmsterdam
T.N.Kipf@uva.nl CanadianInstituteforAdvancedResearch(CIFAR)
M.Welling@uva.nl
ABSTRACT
Wepresentascalableapproachforsemi-supervisedlearningongraph-structured
data thatis basedon anefficient variantof convolutional neuralnetworks which
operate directly on graphs. We motivate the choice of our convolutional archi-
tecture via a localized first-order approximation of spectral graph convolutions.
Our model scales linearly in the number of graph edges and learns hidden layer
representations that encode both local graph structure and features of nodes. In
anumberofexperimentsoncitationnetworksandonaknowledgegraphdataset
we demonstrate that our approach outperforms related methods by a significant
margin.
1 INTRODUCTION
We consider the problem of classifying nodes (such as documents) in a graph (such as a citation
network),wherelabelsareonlyavailableforasmallsubsetofnodes. Thisproblemcanbeframed
as graph-based semi-supervised learning, where label information is smoothed over the graph via
someformofexplicitgraph-basedregularization(Zhuetal.,2003;Zhouetal.,2004;Belkinetal.,
2006;Westonetal.,2012),e.g.byusingagraphLaplacianregularizationterminthelossfunction:
(cid:88)
L=L +λL , with L = A (cid:107)f(X )−f(X )(cid:107)2 =f(X)(cid:62)∆f(X). (1)
0 reg reg ij i j
i,j
Here,L denotesthesupervisedlossw.r.t.thelabeledpartofthegraph,f(·)canbeaneuralnetwork-
0
like differentiable function, λ is a weighing factor and X is a matrix of node feature vectors X .
i
∆ = D −A denotes the unnormalized graph Laplacian of an undirected graph G = (V,E) with
N nodes v ∈ V, edges (v ,v ) ∈ E, an adjacency matrix A ∈ RN×N (binary or weighted) and
i i j
(cid:80)
adegreematrixD = A . TheformulationofEq.1reliesontheassumptionthatconnected
ii j ij
nodes in the graph are likely to share the same label. This assumption, however, might restrict
modeling capacity, as graph edges need not necessarily encode node similarity, but could contain
additionalinformation.
In this work, we encode the graph structure directly using a neural network model f(X,A) and
train on a supervised target L for all nodes with labels, thereby avoiding explicit graph-based
0
regularization in the loss function. Conditioning f(·) on the adjacency matrix of the graph will
allowthemodeltodistributegradientinformationfromthesupervisedlossL andwillenableitto
0
learnrepresentationsofnodesbothwithandwithoutlabels.
Our contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise prop-
agation rule for neural network models which operate directly on graphs and show how it can be
motivatedfromafirst-orderapproximationofspectralgraphconvolutions(Hammondetal.,2011).
Secondly, we demonstrate how this form of a graph-based neural network model can be used for
fast and scalable semi-supervised classification of nodes in a graph. Experiments on a number of
datasets demonstrate that our model compares favorably both in classification accuracy and effi-
ciency(measuredinwall-clocktime)againststate-of-the-artmethodsforsemi-supervisedlearning.
1
PublishedasaconferencepaperatICLR2017
In this way, we can still recover a rich class of convolutional filter functions by stacking multiple
such layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev
polynomials. We intuitively expect that such a model can alleviate the problem of overfitting on
local neighborhood structures for graphs with very wide node degree distributions, such as social
networks,citationnetworks,knowledgegraphsandmanyotherreal-worldgraphdatasets.Addition-
ally, for a fixedcomputational budget, this layer-wise linearformulation allows us to build deeper
models,apracticethatisknowntoimprovemodelingcapacityonanumberofdomains(Heetal.,
2016).
InthislinearformulationofaGCNwefurtherapproximateλ ≈ 2,aswecanexpectthatneural
max
networkparameterswilladapttothischangeinscaleduringtraining. Undertheseapproximations
Eq.5simplifiesto:
g θ(cid:48) (cid:63)x≈θ 0(cid:48)x+θ 1(cid:48) (L−I N)x=θ 0(cid:48)x−θ 1(cid:48)D− 21 AD− 21 x, (6)
with two free parameters θ(cid:48) and θ(cid:48). The filter parameters can be shared over the whole graph.
0 1
Successiveapplicationoffiltersofthisformtheneffectivelyconvolvethekth-orderneighborhoodof
anode,wherekisthenumberofsuccessivefilteringoperationsorconvolutionallayersintheneural
networkmodel.
Inpractice,itcanbebeneficialtoconstrainthenumberofparametersfurthertoaddressoverfitting
andtominimizethenumberofoperations(suchasmatrixmultiplications)perlayer. Thisleavesus
withthefollowingexpression:
(cid:16) (cid:17)
g θ(cid:63)x≈θ I
N
+D− 21 AD− 21 x, (7)
with a single parameter θ = θ 0(cid:48) = −θ 1(cid:48). Note that I
N
+ D− 21AD− 21 now has eigenvalues in
the range [0,2]. Repeated application of this operator can therefore lead to numerical instabilities
and exploding/vanishing gradients when used in a deep neural network model. To alleviate this
problem,weintroducethefollowingrenormalizationtrick:I N+D− 21AD−1 2 →D˜− 21A˜D˜−1 2,with
A˜=A+I andD˜ =(cid:80) A˜ .
N ii j ij
WecangeneralizethisdefinitiontoasignalX ∈RN×C withCinputchannels(i.e.aC-dimensional
featurevectorforeverynode)andF filtersorfeaturemapsasfollows:
Z =D˜−1 2A˜D˜−1 2XΘ, (8)
where Θ ∈ RC×F is now a matrix of filter parameters and Z ∈ RN×F is the convolved signal
matrix. ThisfilteringoperationhascomplexityO(|E|FC), asA˜X canbeefficientlyimplemented
asaproductofasparsematrixwithadensematrix.
3 SEMI-SUPERVISED NODE CLASSIFICATION
Having introduced a simple, yet flexible model f(X,A) for efficient information propagation on
graphs,wecanreturntotheproblemofsemi-supervisednodeclassification. Asoutlinedinthein-
troduction,wecanrelaxcertainassumptionstypicallymadeingraph-basedsemi-supervisedlearn-
ing by conditioning our model f(X,A) both on the data X and on the adjacency matrix A of the
underlyinggraphstructure. Weexpectthissettingtobeespeciallypowerfulinscenarioswherethe
adjacencymatrixcontainsinformationnotpresentinthedataX,suchascitationlinksbetweendoc-
uments in a citation network or relations in a knowledge graph. The overall model, a multi-layer
GCNforsemi-supervisedlearning,isschematicallydepictedinFigure1.
3.1 EXAMPLE
Inthefollowing, weconsideratwo-layerGCNforsemi-supervisednodeclassificationonagraph
withasymmetricadjacencymatrixA(binaryorweighted). WefirstcalculateAˆ = D˜− 21A˜D˜−1 2 in
apre-processingstep. Ourforwardmodelthentakesthesimpleform:
(cid:16) (cid:16) (cid:17) (cid:17)
Z =f(X,A)=softmax Aˆ ReLU AˆXW(0) W(1) . (9)
3
PublishedasaconferencepaperatICLR2017
30
20
C F
X Z Y
1 1 1
10
X Z
2 2 0
hidden
X Z
3 layers 3 10
X Z Y
4 4 4
inputlayer outputlayer 20
(a)GraphConvolutionalNetwork (b)Hiddenlayeractivations
30
Figure1: Left: Schematicdepictionofmulti-layerGraphConv3o0lution2a0lNetw10ork(G0CN)f1o0rsemi2-0 30
supervisedlearningwithC inputchannelsandF featuremapsintheoutputlayer. Thegraphstruc-
ture (edges shown as black lines) is shared over layers, labels are denoted by Y . Right: t-SNE
i
(Maaten & Hinton, 2008) visualization of hidden layer activations of a two-layer GCN trained on
theCoradataset(Senetal.,2008)using5%oflabels. Colorsdenotedocumentclass.
Here,W(0) ∈ RC×H isaninput-to-hiddenweightmatrixforahiddenlayerwithH featuremaps.
W(1) ∈ RH×F is a hidden-to-output weight matrix. The softmax activation function, defined as
softmax(x )= 1 exp(x )withZ =(cid:80) exp(x ),isappliedrow-wise. Forsemi-supervisedmulti-
i Z i i i
classclassification,wethenevaluatethecross-entropyerroroveralllabeledexamples:
F
(cid:88) (cid:88)
L=− Y lnZ , (10)
lf lf
l∈YLf=1
whereY isthesetofnodeindicesthathavelabels.
L
The neural network weights W(0) and W(1) are trained using gradient descent. In this work, we
performbatchgradientdescentusingthefulldatasetforeverytrainingiteration, whichisaviable
optionaslongasdatasetsfitinmemory. UsingasparserepresentationforA,memoryrequirement
isO(|E|),i.e.linearinthenumberofedges. Stochasticityinthetrainingprocessisintroducedvia
dropout(Srivastavaetal.,2014). Weleavememory-efficientextensionswithmini-batchstochastic
gradientdescentforfuturework.
3.2 IMPLEMENTATION
In practice, we make use of TensorFlow (Abadi et al., 2015) for an efficient GPU-based imple-
mentation2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of
evaluatingEq.9isthenO(|E|CHF),i.e.linearinthenumberofgraphedges.
4 RELATED WORK
Ourmodeldrawsinspirationbothfromthefieldofgraph-basedsemi-supervisedlearningandfrom
recentworkonneuralnetworksthatoperateongraphs.Inwhatfollows,weprovideabriefoverview
onrelatedworkinbothfields. 1
4.1 GRAPH-BASEDSEMI-SUPERVISEDLEARNING
Alargenumberofapproachesforsemi-supervisedlearningusinggraphrepresentationshavebeen
proposed in recent years, most of which fall into two broad categories: methods that use some
formofexplicitgraphLaplacianregularizationandgraphembedding-basedapproaches. Prominent
examplesforgraphLaplacianregularizationincludelabelpropagation(Zhuetal.,2003),manifold
regularization(Belkinetal.,2006)anddeepsemi-supervisedembedding(Westonetal.,2012).
2Codetoreproduceourexperimentsisavailableathttps://github.com/tkipf/gcn.
4
PublishedasaconferencepaperatICLR2017
Recently, attention has shifted to models that learn graph embeddings with methods inspired by
the skip-gram model (Mikolov et al., 2013). DeepWalk (Perozzi et al., 2014) learns embeddings
via the prediction of the local neighborhood of nodes, sampled from random walks on the graph.
LINE (Tang et al., 2015) and node2vec (Grover & Leskovec, 2016) extend DeepWalk with more
sophisticatedrandomwalkorbreadth-firstsearchschemes. Forallthesemethods,however,amulti-
steppipelineincludingrandomwalkgenerationandsemi-supervisedtrainingisrequiredwhereeach
step has to be optimized separately. Planetoid (Yang et al., 2016) alleviates this by injecting label
informationintheprocessoflearningembeddings.
4.2 NEURALNETWORKSONGRAPHS
Neural networks that operate on graphs have previously been introduced in Gori et al. (2005);
Scarsellietal.(2009)asaformofrecurrentneuralnetwork. Theirframeworkrequirestherepeated
application of contraction maps as propagation functions until node representations reach a stable
fixedpoint. ThisrestrictionwaslateralleviatedinLietal.(2016)byintroducingmodernpractices
for recurrent neural network training to the original graph neural network framework. Duvenaud
etal.(2015)introducedaconvolution-likepropagationruleongraphsandmethodsforgraph-level
classification. Theirapproachrequirestolearnnodedegree-specificweightmatriceswhichdoesnot
scaletolargegraphswithwidenodedegreedistributions. Ourmodelinsteadusesasingleweight
matrix per layer and deals with varying node degrees through an appropriate normalization of the
adjacencymatrix(seeSection3.1).
Arelatedapproachtonodeclassificationwithagraph-basedneuralnetworkwasrecentlyintroduced
inAtwood&Towsley(2016). TheyreportO(N2)complexity,limitingtherangeofpossibleappli-
cations. Inadifferentyetrelatedmodel,Niepertetal.(2016)convertgraphslocallyintosequences
thatarefedintoaconventional1Dconvolutionalneuralnetwork,whichrequiresthedefinitionofa
nodeorderinginapre-processingstep.
Our method is based on spectral graph convolutional neural networks, introduced in Bruna et al.
(2014) and later extended by Defferrard et al. (2016) with fast localized convolutions. In contrast
to these works, we consider here the task of transductive node classification within networks of
significantlylargerscale. Weshowthatinthissetting,anumberofsimplifications(seeSection2.2)
canbeintroducedtotheoriginalframeworksofBrunaetal.(2014)andDefferrardetal.(2016)that
improvescalabilityandclassificationperformanceinlarge-scalenetworks.
5 EXPERIMENTS
We test our model in a number of experiments: semi-supervised document classification in cita-
tionnetworks,semi-supervisedentityclassificationinabipartitegraphextractedfromaknowledge
graph,anevaluationofvariousgraphpropagationmodelsandarun-timeanalysisonrandomgraphs.
5.1 DATASETS
We closely follow the experimental setup in Yang et al. (2016). Dataset statistics are summarized
inTable1. Inthecitationnetworkdatasets—Citeseer,CoraandPubmed(Senetal.,2008)—nodes
aredocumentsandedgesarecitationlinks. Labelratedenotesthenumberoflabelednodesthatare
usedfortrainingdividedbythetotalnumberofnodesineachdataset. NELL(Carlsonetal.,2010;
Yangetal.,2016)isabipartitegraphdatasetextractedfromaknowledgegraphwith55,864relation
nodesand9,891entitynodes.
Table1: Datasetstatistics,asreportedinYangetal.(2016).
Dataset Type Nodes Edges Classes Features Labelrate
Citeseer Citationnetwork 3,327 4,732 6 3,703 0.036
Cora Citationnetwork 2,708 5,429 7 1,433 0.052
Pubmed Citationnetwork 19,717 44,338 3 500 0.003
NELL Knowledgegraph 65,755 266,144 210 5,414 0.001
5
PublishedasaconferencepaperatICLR2017
6 RESULTS
6.1 SEMI-SUPERVISEDNODECLASSIFICATION
ResultsaresummarizedinTable2. Reportednumbersdenoteclassificationaccuracyinpercent. For
ICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other
baselinemethodsaretakenfromthePlanetoidpaper(Yangetal.,2016).Planetoid*denotesthebest
modelfortherespectivedatasetoutofthevariantspresentedintheirpaper.
Table2: Summaryofresultsintermsofclassificationaccuracy(inpercent).
Method Citeseer Cora Pubmed NELL
ManiReg[3] 60.1 59.5 70.7 21.8
SemiEmb[28] 59.6 59.0 71.1 26.7
LP[32] 45.3 68.0 63.0 26.5
DeepWalk[22] 43.2 67.2 65.3 58.1
ICA[18] 69.1 75.1 73.9 23.1
Planetoid*[29] 64.7(26s) 75.7(13s) 77.2(25s) 61.9(185s)
GCN(thispaper) 70.3(7s) 81.5(4s) 79.0(38s) 66.0(48s)
GCN(rand.splits) 67.9±0.5 80.1±0.5 78.9±0.7 58.4±1.7
Wefurtherreportwall-clocktrainingtimeinsecondsuntilconvergence(inbrackets)forourmethod
(incl.evaluationofvalidationerror)andforPlanetoid.Forthelatter,weusedanimplementationpro-
videdbytheauthors3andtrainedonthesamehardware(withGPU)asourGCNmodel. Wetrained
andtestedourmodelonthesamedatasetsplitsasinYangetal.(2016)andreportmeanaccuracy
of100runswithrandomweightinitializations. Weusedthefollowingsetsofhyperparametersfor
Citeseer,CoraandPubmed: 0.5(dropoutrate),5·10−4 (L2regularization)and16(numberofhid-
denunits);andforNELL:0.1(dropoutrate),1·10−5(L2regularization)and64(numberofhidden
units).
Inaddition, wereportperformanceofourmodelon10randomlydrawndatasetsplitsofthesame
size as in Yang et al. (2016), denoted by GCN (rand. splits). Here, we report mean and standard
errorofpredictionaccuracyonthetestsetsplitinpercent.
6.2 EVALUATIONOFPROPAGATIONMODEL
Wecomparedifferentvariantsofourproposedper-layerpropagationmodelonthecitationnetwork
datasets. Wefollowtheexperimentalset-updescribedintheprevioussection. Resultsaresumma-
rizedinTable3. ThepropagationmodelofouroriginalGCNmodelisdenotedbyrenormalization
trick (inbold). Inallothercases, thepropagationmodelofbothneuralnetworklayersisreplaced
with the model specified under propagation model. Reported numbers denote mean classification
accuracyfor100repeatedrunswithrandomweightmatrixinitializations. Incaseofmultiplevari-
ablesΘ perlayer,weimposeL2regularizationonallweightmatricesofthefirstlayer.
i
Table3: Comparisonofpropagationmodels.
Description Propagationmodel Citeseer Cora Pubmed
Chebyshevfilter(Eq.5) K =3 (cid:80)K T (L˜)XΘ 69.8 79.5 74.4
K =2 k=0 k k 69.6 81.2 73.8
1st-ordermodel(Eq.6) XΘ 0+D− 21AD−1 2XΘ
1
68.3 80.0 77.5
Singleparameter(Eq.7) (I
N
+D−1 2AD− 21)XΘ 69.3 79.2 77.4
Renormalizationtrick(Eq.8) D˜−1 2A˜D˜− 21XΘ 70.3 81.5 79.0
1st-ordertermonly D−1 2AD− 21XΘ 68.7 80.5 77.8
Multi-layerperceptron XΘ 46.5 55.1 71.4
3https://github.com/kimiyoung/planetoid
7
PublishedasaconferencepaperatICLR2017
6.3 TRAININGTIMEPEREPOCH
101
100
10-1
10-2
10-3
1k 10k 100k 1M 10M
# Edges
hcope/.ceS
Here, we report results for the mean training GPU
time per epoch (forward pass, cross-entropy CPU
calculation, backward pass) for 100 epochs on
simulatedrandomgraphs,measuredinseconds
wall-clocktime. SeeSection5.1foradetailed
*
description of the random graph dataset used
in these experiments. We compare results on
aGPUandonaCPU-onlyimplementation4 in
TensorFlow(Abadietal.,2015). Figure2sum- Figure 2: Wall-clock time per epoch for random
marizestheresults. graphs. (*)indicatesout-of-memoryerror.
7 DISCUSSION
7.1 SEMI-SUPERVISEDMODEL
In the experiments demonstrated here, our method for semi-supervised node classification outper-
formsrecentrelatedmethodsbyasignificantmargin. Methodsbasedongraph-Laplacianregular-
ization(Zhuetal.,2003;Belkinetal.,2006;Westonetal.,2012)aremostlikelylimitedduetotheir
assumptionthatedgesencodemeresimilarityofnodes. Skip-grambasedmethodsontheotherhand
are limited by the fact that they are based on a multi-step pipeline which is difficult to optimize.
Ourproposedmodelcanovercomebothlimitations,whilestillcomparingfavorablyintermsofef-
ficiency(measuredinwall-clocktime)torelatedmethods. Propagationoffeatureinformationfrom
neighboringnodesineverylayerimprovesclassificationperformanceincomparisontomethodslike
ICA(Lu&Getoor,2003),whereonlylabelinformationisaggregated.
Wehavefurtherdemonstratedthattheproposedrenormalizedpropagationmodel(Eq.8)offersboth
improvedefficiency(fewerparametersandoperations,suchasmultiplicationoraddition)andbetter
predictive performance on a number of datasets compared to a na¨ıve 1st-order model (Eq. 6) or
higher-ordergraphconvolutionalmodelsusingChebyshevpolynomials(Eq.5).
7.2 LIMITATIONSANDFUTUREWORK
Here,wedescribeseverallimitationsofourcurrentmodelandoutlinehowthesemightbeovercome
infuturework.
Memoryrequirement Inthecurrentsetupwithfull-batchgradientdescent,memoryrequirement
growslinearlyinthesizeofthedataset. WehaveshownthatforlargegraphsthatdonotfitinGPU
memory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can
alleviatethisissue.Theprocedureofgeneratingmini-batches,however,shouldtakeintoaccountthe
numberoflayersintheGCNmodel,astheKth-orderneighborhoodforaGCNwithKlayershasto
bestoredinmemoryforanexactprocedure. Forverylargeanddenselyconnectedgraphdatasets,
furtherapproximationsmightbenecessary.
Directededgesandedgefeatures Ourframeworkcurrentlydoesnotnaturallysupportedgefea-
tures and is limited to undirected graphs (weighted or unweighted). Results on NELL however
showthatitispossibletohandlebothdirectededgesandedgefeaturesbyrepresentingtheoriginal
directed graph as an undirected bipartite graph with additional nodes that represent edges in the
originalgraph(seeSection5.1fordetails).
Limitingassumptions ThroughtheapproximationsintroducedinSection2,weimplicitlyassume
locality (dependence on the Kth-order neighborhood for a GCN with K layers) and equal impor-
tanceofself-connectionsvs.edgestoneighboringnodes. Forsomedatasets, however, itmightbe
beneficialtointroduceatrade-offparameterλinthedefinitionofA˜:
A˜=A+λI . (11)
N
4Hardwareused:16-coreIntel(cid:13)R Xeon(cid:13)R CPUE5-2640v3@2.60GHz,GeForce(cid:13)R GTXTITANX
8
PublishedasaconferencepaperatICLR2017
Thisparameternowplaysasimilarroleasthetrade-offparameterbetweensupervisedandunsuper-
vised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via
gradientdescent.
8 CONCLUSION
We have introduced a novel approach for semi-supervised classification on graph-structured data.
OurGCNmodelusesanefficientlayer-wisepropagationrulethatisbasedonafirst-orderapprox-
imationofspectralconvolutionsongraphs. Experimentsonanumberofnetworkdatasetssuggest
that the proposed GCN model is capable of encoding both graph structure and node features in a
wayusefulforsemi-supervisedclassification.Inthissetting,ourmodeloutperformsseveralrecently
proposedmethodsbyasignificantmargin,whilebeingcomputationallyefficient.
ACKNOWLEDGMENTS
We would like to thank Christos Louizos, Taco Cohen, Joan Bruna, Zhilin Yang, Dave Herman,
PramodSinhaandAbdul-SaboorSheikhforhelpfuldiscussions. ThisresearchwasfundedbySAP.
REFERENCES
Mart´ınAbadietal. TensorFlow: Large-scalemachinelearningonheterogeneoussystems,2015.
JamesAtwoodandDonTowsley. Diffusion-convolutionalneuralnetworks. InAdvancesinneural
informationprocessingsystems(NIPS),2016.
MikhailBelkin,ParthaNiyogi,andVikasSindhwani. Manifoldregularization: Ageometricframe-
work for learning from labeled and unlabeled examples. Journal of machine learning research
(JMLR),7(Nov):2399–2434,2006.
Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski,
and Dorothea Wagner. On modularity clustering. IEEE Transactions on Knowledge and Data
Engineering,20(2):172–188,2008.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connectednetworksongraphs.InInternationalConferenceonLearningRepresentations(ICLR),
2014.
AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.HruschkaJr,andTomM.
Mitchell. Towardanarchitecturefornever-endinglanguagelearning. InAAAI,volume5,pp. 3,
2010.
Michae¨lDefferrard,XavierBresson,andPierreVandergheynst. Convolutionalneuralnetworkson
graphswithfastlocalizedspectralfiltering.InAdvancesinneuralinformationprocessingsystems
(NIPS),2016.
BrendanL.Douglas.TheWeisfeiler-Lehmanmethodandgraphisomorphismtesting.arXivpreprint
arXiv:1101.5211,2011.
DavidK.Duvenaud,DougalMaclaurin,JorgeIparraguirre,RafaelBombarell,TimothyHirzel,Ala´n
Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular
fingerprints.InAdvancesinneuralinformationprocessingsystems(NIPS),pp.2224–2232,2015.
XavierGlorotandYoshuaBengio. Understandingthedifficultyoftrainingdeepfeedforwardneural
networks. InAISTATS,volume9,pp.249–256,2010.
MarcoGori,GabrieleMonfardini,andFrancoScarselli.Anewmodelforlearningingraphdomains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks., volume 2, pp.
729–734.IEEE,2005.
AdityaGroverandJureLeskovec.node2vec:Scalablefeaturelearningfornetworks.InProceedings
ofthe22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining.
ACM,2016.
9
PublishedasaconferencepaperatICLR2017
David K. Hammond, Pierre Vandergheynst, and Re´mi Gribonval. Wavelets on graphs via spectral
graphtheory. AppliedandComputationalHarmonicAnalysis,30(2):129–150,2011.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016.
ThorstenJoachims. Transductiveinferencefortextclassificationusingsupportvectormachines. In
InternationalConferenceonMachineLearning(ICML),volume99,pp.200–209,1999.
DiederikP.KingmaandJimmyLeiBa. Adam: Amethodforstochasticoptimization. InInterna-
tionalConferenceonLearningRepresentations(ICLR),2015.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. InInternationalConferenceonLearningRepresentations(ICLR),2016.
QingLuandLiseGetoor.Link-basedclassification.InInternationalConferenceonMachineLearn-
ing(ICML),volume3,pp.496–503,2003.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
LearningResearch(JMLR),9(Nov):2579–2605,2008.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in neural information
processingsystems(NIPS),pp.3111–3119,2013.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
worksforgraphs. InInternationalConferenceonMachineLearning(ICML),2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discoveryanddatamining,pp.701–710.ACM,2014.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
Thegraphneuralnetworkmodel. IEEETransactionsonNeuralNetworks,20(1):61–80,2009.
PrithvirajSen,GalileoNamata,MustafaBilgic,LiseGetoor,BrianGalligher,andTinaEliassi-Rad.
Collectiveclassificationinnetworkdata. AImagazine,29(3):93,2008.
NitishSrivastava,GeoffreyE.Hinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.
Dropout:asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachineLearning
Research(JMLR),15(1):1929–1958,2014.
JianTang,MengQu,MingzheWang,MingZhang,JunYan,andQiaozhuMei. Line: Large-scale
informationnetworkembedding. InProceedingsofthe24thInternationalConferenceonWorld
WideWeb,pp.1067–1077.ACM,2015.
Boris Weisfeiler and A. A. Lehmann. A reduction of a graph to a canonical form and an algebra
arisingduringthisreduction. Nauchno-TechnicheskayaInformatsia,2(9):12–16,1968.
Jason Weston, Fre´de´ric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervisedembedding. InNeuralNetworks: TricksoftheTrade,pp.639–655.Springer,2012.
ZhilinYang,WilliamCohen,andRuslanSalakhutdinov. Revisitingsemi-supervisedlearningwith
graphembeddings. InInternationalConferenceonMachineLearning(ICML),2016.
WayneW.Zachary. Aninformationflowmodelforconflictandfissioninsmallgroups. Journalof
anthropologicalresearch,pp.452–473,1977.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scho¨lkopf.
Learningwithlocalandglobalconsistency.InAdvancesinneuralinformationprocessingsystems
(NIPS),volume16,pp.321–328,2004.
XiaojinZhu,ZoubinGhahramani,andJohnLafferty.Semi-supervisedlearningusinggaussianfields
and harmonic functions. In International Conference on Machine Learning (ICML), volume 3,
pp.912–919,2003.
10
PublishedasaconferencepaperatICLR2017
A RELATION TO WEISFEILER-LEHMAN ALGORITHM
Aneuralnetworkmodelforgraph-structureddatashouldideallybeabletolearnrepresentationsof
nodesinagraph, takingboththegraphstructureandfeaturedescriptionofnodesintoaccount. A
well-studiedframeworkfortheuniqueassignmentofnodelabelsgivenagraphand(optionally)dis-
creteinitialnodelabelsisprovidedbythe1-dimWeisfeiler-Lehman(WL-1)algorithm(Weisfeiler
&Lehmann,1968):
Algorithm1:WL-1algorithm(Weisfeiler&Lehmann,1968)
Input: Initialnodecoloring(h(0),h(0),...,h(0))
1 2 N
Output: Finalnodecoloring(h(T),h(T),...,h(T))
1 2 N
t←0;
repeat
forv ∈V do
i
(cid:16) (cid:17)
h(t+1) ←hash (cid:80) h(t) ;
i j∈Ni j
t←t+1;
untilstablenodecoloringisreached;
Here, h(t) denotes the coloring (label assignment) of node v (at iteration t) and N is its set of
i i i
neighboringnodeindices(irrespectiveofwhetherthegraphincludesself-connectionsforeverynode
ornot). hash(·)isahashfunction. Foranin-depthmathematicaldiscussionoftheWL-1algorithm
see,e.g.,Douglas(2011).
We can replace the hash function in Algorithm 1 with a neural network layer-like differentiable
functionwithtrainableparametersasfollows:
 
h(l+1) =σ(cid:88) 1 h(l)W(l)  , (12)
i c j
ij
j∈Ni
wherec isanappropriatelychosennormalizationconstantfortheedge(v ,v ). Further, wecan
ij i j
take h(l) now to be a vector of activations of node i in the lth neural network layer. W(l) is a
i
layer-specificweightmatrixandσ(·)denotesadifferentiable,non-linearactivationfunction.
(cid:112)
Bychoosingc = d d ,whered =|N |denotesthedegreeofnodev ,werecoverthepropaga-
ij i j i i i
tionruleofourGraphConvolutionalNetwork(GCN)modelinvectorform(seeEq.2)5.
This—looselyspeaking—allowsustointerpretourGCNmodelasadifferentiableandparameter-
izedgeneralizationofthe1-dimWeisfeiler-Lehmanalgorithmongraphs.
A.1 NODEEMBEDDINGSWITHRANDOMWEIGHTS
FromtheanalogywiththeWeisfeiler-Lehmanalgorithm,wecanunderstandthatevenanuntrained
GCNmodelwithrandomweightscanserveasapowerfulfeatureextractorfornodesinagraph. As
anexample,considerthefollowing3-layerGCNmodel:
(cid:16) (cid:16) (cid:16) (cid:17) (cid:17) (cid:17)
Z =tanh Aˆ tanh Aˆ tanh AˆXW(0) W(1) W(2) , (13)
withweightmatricesW(l)initializedatrandomusingtheinitializationdescribedinGlorot&Bengio
(2010). Aˆ,X andZ aredefinedasinSection3.1.
We apply this model on Zachary’s karate club network (Zachary, 1977). This graph contains 34
nodes, connected by 154 (undirected and unweighted) edges. Every node is labeled by one of
fourclasses,obtainedviamodularity-basedclustering(Brandesetal.,2008). SeeFigure3aforan
illustration.
5Notethatwehereimplicitlyassumethatself-connectionshavealreadybeenaddedtoeverynodeinthe
graph(foraclutter-freenotation).
11
PublishedasaconferencepaperatICLR2017
(a)Karateclubnetwork (b)Randomweightembedding
Figure3: Left: Zachary’skarateclubnetwork(Zachary,1977),colorsdenotecommunitiesobtained
via modularity-based clustering (Brandes et al., 2008). Right: Embeddings obtained from an un-
trained3-layerGCNmodel(Eq.13)withrandomweightsappliedtothekarateclubnetwork. Best
viewedonacomputerscreen.
WetakeafeaturelessapproachbysettingX = I ,whereI istheN byN identitymatrix. N is
N N
thenumberofnodesinthegraph. Notethatnodesarerandomlyordered(i.e.orderingcontainsno
information). Furthermore, we choose a hidden layer dimensionality6 of 4 and a two-dimensional
output(sothattheoutputcanimmediatelybevisualizedina2-dimplot).
Figure 3b shows a representative example of node embeddings (outputs Z) obtained from an un-
trainedGCNmodelappliedtothekarateclubnetwork. Theseresultsarecomparabletoembeddings
obtainedfromDeepWalk(Perozzietal.,2014),whichusesamoreexpensiveunsupervisedtraining
procedure.
A.2 SEMI-SUPERVISEDNODEEMBEDDINGS
OnthissimpleexampleofaGCNappliedtothekarateclubnetworkitisinterestingtoobservehow
embeddingsreactduringtrainingonasemi-supervisedclassificationtask. Suchavisualization(see
Figure 4) provides insights into how the GCN model can make use of the graph structure (and of
featuresextractedfromthegraphstructureatlaterlayers)tolearnembeddingsthatareusefulfora
classificationtask.
We consider the following semi-supervised learning setup: we add a softmax layer on top of our
model(Eq.13)andtrainusingonlyasinglelabeledexampleperclass(i.e.atotalnumberof4labeled
nodes). Wetrainfor300trainingiterationsusingAdam(Kingma&Ba,2015)withalearningrate
of0.01onacross-entropyloss.
Figure4showstheevolutionofnodeembeddingsoveranumberoftrainingiterations. Themodel
succeedsinlinearlyseparatingthecommunitiesbasedonminimalsupervisionandthegraphstruc-
turealone. Avideoofthefulltrainingprocesscanbefoundonourwebsite7.
6Weoriginallyexperimentedwithahiddenlayerdimensionalityof2(i.e.sameasoutputlayer),butobserved
that a dimensionality of 4 resulted in less frequent saturation of tanh(·) units and therefore visually more
pleasingresults.
7http://tkipf.github.io/graph-convolutional-networks/
12
PublishedasaconferencepaperatICLR2017
(a)Iteration25 (b)Iteration50
(c)Iteration75 (d)Iteration100
(e)Iteration200 (f)Iteration300
Figure 4: Evolution of karate club network node embeddings obtained from a GCN model after a
number of semi-supervised training iterations. Colors denote class. Nodes of which labels were
provided during training (one per class) are highlighted (grey outline). Grey links between nodes
denotegraphedges. Bestviewedonacomputerscreen.
13
PublishedasaconferencepaperatICLR2017
B EXPERIMENTS ON MODEL DEPTH
Intheseexperiments,weinvestigatetheinfluenceofmodeldepth(numberoflayers)onclassification
performance. We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and
Pubmeddatasets(Senetal.,2008)usingalllabels. InadditiontothestandardGCNmodel(Eq.2),
we report results on a model variant where we use residual connections (He et al., 2016) between
hiddenlayerstofacilitatetrainingofdeepermodelsbyenablingthemodeltocarryoverinformation
fromthepreviouslayer’sinput:
(cid:16) (cid:17)
H(l+1) =σ D˜− 21A˜D˜− 21H(l)W(l) +H(l). (14)
On each cross-validation split, we train for 400 epochs (without early stopping) using the Adam
optimizer(Kingma&Ba,2015)withalearningrateof0.01. Otherhyperparametersarechosenas
follows: 0.5(dropoutrate,firstandlastlayer),5·10−4 (L2regularization,firstlayer),16(number
ofunitsforeachhiddenlayer)and0.01(learningrate). ResultsaresummarizedinFigure5.
0.90
0.85
0.80
0.75
0.70
0.65
0.60
0.55
0.50
1 2 3 4 5 6 7 8 9 10
Number of layers
ycaruccA
Citeseer
0.95
0.90
0.85
0.80
0.75
0.70
Train 0.65
Train (Residual)
Test 0.60
Test (Residual)
0.55
1 2 3 4 5 6 7 8 9 10
Number of layers
ycaruccA
Cora
0.88
0.86
0.84
0.82
0.80
Train 0.78
Train (Residual)
Test 0.76
Test (Residual)
1 2 3 4 5 6 7 8 9 10
Number of layers
ycaruccA
Pubmed
Train
Train (Residual)
Test
Test (Residual)
Figure 5: Influence of model depth (number of layers) on classification performance. Markers
denotemeanclassificationaccuracy(trainingvs.testing)for5-foldcross-validation. Shadedareas
denotestandarderror. WeshowresultsbothforastandardGCNmodel(dashedlines)andamodel
withaddedresidualconnections(Heetal.,2016)betweenhiddenlayers(solidlines).
Forthedatasetsconsideredhere, bestresultsareobtainedwitha2-or3-layermodel. Weobserve
that for models deeper than 7 layers, training without the use of residual connections can become
difficult,astheeffectivecontextsizeforeachnodeincreasesbythesizeofitsKth-orderneighbor-
hood(foramodelwithK layers)witheachadditionallayer. Furthermore,overfittingcanbecome
anissueasthenumberofparametersincreaseswithmodeldepth.
14

