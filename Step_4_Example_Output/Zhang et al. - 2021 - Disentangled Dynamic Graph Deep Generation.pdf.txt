References:
(a)Groundtruth (b)z,z(cid:48) andz(cid:48)(cid:48) (withoutf) (c)fandcombinedz,z(cid:48) andz(cid:48)(cid:48)
Figure 4: Visualization of graphs from Protein dataset and generated by all ablated methods.
topology; when z(cid:48)(cid:48) is the only varying factor, node and
edges change across different graphs in Figure 3(d) as
composed to either edges change in Figure 3(b) or node
changes in Figure 3(c).
Ablation Studies. We also perform two ablations
to confirm the necessity of the proposed factorization
design. Without time-independent latent representa-
tion, the graphs generated in Figure 4(b) capture the
underlying node characteristics but not topology. This
might be because although the latent variables increase
by respective dimension of ablated f, the dimension is
still too small to fully capture its topology (yet intrinsic)
characteristics. On the other hand, without separated
Figure 5: Scalability comparison of all the methods.
topology and node latent variables, Figure 4(c) reveals
that the generated topology looks close to ground truth
graph generation as a disentanglement problem and
but the node characteristics do not. This illustrates
present a model consisting of two derived penalties for
the necessity of separating topology and node represen-
disentanglementsbetweentopologyandattributeaswell
tations. As otherwise the model might focus on the
as between time-dependent and time-varying factors.
learning of more complex topology representation and
The experimental evaluation results show the flexibility
ignore node representation, or vice versa.
and versatility of D2G2 in generating dynamic graphs.
Scalability. We also illustrate and analyze the
Oneimmediatefuturedirectionistoextendtheseresults
scalability in the number of nodes of our D2G2 method
in conjunction with our previous works [37, 38] for
against comparison methods, as shown in Figure 5,
unbiased scene graph generation.
where the y-axis is the logarithm (base 10) of the
runtime in seconds. As can be seen, our method
Acknowledgement
achieves competitive scalability in general, which is
This work was supported by the National Science
similar to GraphRNN and better than GraphVAE and
Foundation(NSF)GrantNo. 1755850,No. 1841520,No.
DSGM. NetGAN is known for advantageous scalability
2007716, No. 2007976, No. 1942594, No. 1907805, a
yetlowerinterpretabilityduetotheutilizationofrandom
JeffressMemorialTrustAward,AmazonResearchAward,
walks. GraphVAEandDSGMcannotrunwithaffordable
NVIDIA GPU Grant, and Design Knowledge Company memory and time for graphs larger than 1000 nodes in
(subcontract number: 10827.002.120.04).
our experiments.
5 Conclusions
References
Generative models for real-world graphs have attracted
significant attentions since it is an attractive way to
learn high-level graph representations. However, most [1] X. Guo and L. Zhao, “A systematic survey on deep
of existing graph generative models do not explicitly generativemodelsforgraphgeneration,” arXivpreprint
considerthedynamicallyevolvingtopologyandattribute arXiv:2007.06686, 2020.
information. To fill the gap, this paper proposes a novel [2] S. M. Kazemi, R. Goel, K. Jain, I. Kobyzev, A. Sethi,
deep generative model, known as D2G2, for attributed P. Forsyth, and P. Poupart, “Relational representation
learning for dynamic (knowledge) graphs: A survey,”
temporal graphs. We consider the attributed temporal
arXiv preprint arXiv:1905.11485, 2019.
Copyright©2021bySIAM
745 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
.
511.042.081.041
ot
42/41/20
dedaolnwoD

Annexes/Appendices:

Body Text:
Disentangled Dynamic Graph Deep Generation
Wenbin Zhang∗ Liming Zhang † Dieter Pfoser † Liang Zhao1‡
Abstract containing the graph generation rules or distribution.
Deepgenerativemodelsforgraphshaveexhibitedpromis- Graph generation is an important domain with long
ing performance in ever-increasing domains such as de- history that attracts extensive models such as random
sign of molecules (i.e, graph of atoms) and structure graphs [3], small world models [4], scale-free graphs [5],
prediction of proteins (i.e., graph of amino acids). Exist- and stochastic block models [6]. They typically rely on
ing work typically focuses on static rather than dynamic the network generation principles predefined by human
graphs, which are actually very important in the ap- heuristicsandpriorknowledge,whicheffectivelyabstract
plications such as protein folding, molecule reactions, the high-dimension problems down to manageable scale.
andhumanmobility. Extendingexistingdeepgenerative Such methods usually fit well towards the properties
models from static to dynamic graphs is a challenging that the predefined principles are tailored for, but usu-
task, which requires to handle the factorization of static ally cannot do well for the others [7]. Unfortunately, the
and dynamic characteristics as well as mutual interac- underlyingprincipleofmanycriticalreal-worldproblems
tions among node and edge patterns. Here, this paper is still unknown, such as generating new molecule un-
proposes a novel framework of factorized deep gener- der desired biophysical properties and simulating brain
ative models to achieve interpretable dynamic graph functional connectivity.
generation. Various generative models are proposed to Inrecentyears,thesuccessofdeepgenerativemodels
characterizeconditionalindependenceamongnode,edge, in image and text generation [8, 9] has been extended
static, and dynamic factors. Then, variational optimiza- to graph data applications such as molecule design [10]
tion strategies as well as dynamic graph decoders are and protein structure prediction [11]. They typically
proposed based on newly designed factorized variational leverage frameworks such as Variatioal Autoencoder
autoencoders and recurrent graph deconvolutions. Ex- (VAE) [12] and Generative Adversarial Nets (GAN) [13],
tensive experiments on multiple datasets demonstrate which rely on highly expressive deep architectures to
the effectiveness of the proposed models. map high-dimensional graph-structured data into latent
low-dimensional space, where the latent data points just
follow simple distributions. However, existing works
1 Introduction
on deep generative models of graphs typically focus
Graphsareubiquitousdatastructurestocaptureconnec-
on static graphs instead of dynamic graphs. Although
tions (i.e., edges) among individual units (i.e., nodes),
researchondynamicnetworksisatrendingdeeplearning
such as social networks, proteins (network of amino
topic, it mostly focuses on representation learning tasks,
acids), and molecules (network of atoms). One central
including node/graph embedding [7, 14], node/graph
problem in data mining and machine learning for graphs
classification [15, 16], link prediction [17, 18]. However,
is the gap between the discrete graph topological infor-
thedomainofdeepgenerativemodelsfordynamicgraph
mation and continuous numerical vectors preferred by
generation has not been well explored [1, 2].
mathematical models [1]. This directly leads to two ma-
Extending the current deep generative models from
jordirectionsongraphresearch: 1)graphrepresentation
static graphs to dynamic graphs are very important
learning [2], which aims at encoding graph structural
to many critical domains, such as modeling protein
information into (low-dimensional) vector space, and 2)
folding process [10], human mobility networks [19],
graph generation [1], which reversely aims at construct-
and dynamic functional connectivity process in human
ing a graph-structured data from low-dimensional space
brains [20]. Despite its enormous importance, several
major challenges hinder this tasks from being easily
∗UniversityofMaryland,BaltimoreCounty,MD21250,USA. achievedbyexistingtechniques,including: 1)Difficulty
Email: wenbinzhang@umbc.edu in jointly modeling node dynamics and edge
†GeorgeMasonuniversity,VA22030,USA. dynamics. Nodes could involve continuous values in
‡Emory University, GA 30322, USA. Email: theirattributeswhileedgescouldinvolvegraphtopology
liang.zhao@emory.edu
information that is discrete. Some node dynamic
1correspondingauthor
Copyright©2021bySIAM
738 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
. 511.042.081.041
ot
42/41/20
dedaolnwoD
patterns and edge dynamic patterns could be coupled fit well towards the properties that the predefined
with each other while the other of their patterns may be principles are tailored for, but usually cannot do well for
independent. It is difficult to build a generic, expressive the others [7].
modelsthatcanautomaticallylearnandstratifyallsuch Representationlearningondynamicgraphs. Fast
patterns from data. 2) Difficulty in characterizing increase attentions have been attracted in this research
dynamic and static components. A dynamic graph direction to encode graph’s structural properties into
is typically a mixture of both time-evolving patterns nodes, edges, or the whole graph embeddings. Rep-
as well as stationary patterns. For example, during resentative works include temporal sequence embed-
protein folding, its backbone connection tends to be ding [23], graph sequence embedding [24] and more
stable as a chain structure while other connections such recently continuous-time embedding [14]. A compre-
as hydrogen bonding could vary over time. How deep hensive recent literature survey covering this research
generative models can identify, disentangle, and model effort is provided in [2].
staticanddynamicpatternsindynamicgraphsisanopen Deep generative models for the graphs. Deep
problem. 3) Lack of graph decoders for dynamic generative neural networks have achieved the state
node and edge joint generations. Deep generative of the art results for graph generation in various
models require decoding latent representations back domains,suchasmoleculedesign[10]andcyber-network
to graph domain, which itself is a nascent, open, and synthesis [25]. Most of the existing works in this thread
promising domain. How to further extend it to dynamic are based on variational autoencoders (VAE) [26] and
graph decoding is deemed even more interesting and generativeadversarialnetworks(GAN)[9]. Forexample,
challenging, which requires to further consider the GraphVAE [10] utilizes the VAE model to learn the
temporal dependency [2]. representation of graphs but also node features; while
Toaddresstheabovechallenges,thispaperproposes GraphRNN [27] decomposes the graph generation into
a novel framework of Disentangled deep generative mod- a sequence of node and edge formations that can be
elsforinterpretableDynamicGraphGeneration(D2G2). learned by autoregressive models. NetGAN [13] and
New generative models are proposed to characterize con- its extensions [1] are trained with the GAN algorithm
ditional independence among node, edge, static, and to generate synthetic random walks while discriminates
dynamic factors. Then, varational optimization strate- synthetic walks from real random walks sampled from a
gies are proposed based on newly designed factorized real graph. The critical limitation of existing works is
variational autoencoders. Third, novel dynamic node- that they majorly focus on static networks and cannot
edge co-decoders are proposed based on recurrent and disentangle the dynamic and static patterns for nodes
graph deconvolutions. The contributions of this paper and edges.
can be summarized as follows: Disentangled generative models. Disentangled rep-
resentation learning aims at learning distinguishable
• A generic framework of deep generative models for
underlying representations that responsible for differ-
interpretable dynamic graph generation.
ent variations in the data. Such representations have
been utilized to improve generalizability [28] and inter- • New deep graph models that jointly characterize
pretability [29]. A surge of models have been proposed
node, edge, static, and dynamic factors.
for extending the training objective to enhance disentan-
• New dynamic graph factorized varational autoen- glement in various deep generative model architecture
coders for inferring the established graph models. such as varational autoencoders and generative adver-
sarial nets [30, 31]. However, most existing works inves-
• New dynamic graph decoder architecture to jointly
tigate in the field of image representation learning and
decode time-evolving nodes and edges.
the disentanglement of the latent factors behind a graph
• Extensive experiments and case studies. has not been well explored. Our approach fills this gap
and further addresses dynamic graph generation.
2 Related Work
3 Disentangled Dynamic Graph Deep
Temporal graph generation. Research on temporal
Generation (D2G2)
graphgenerationhasmainlyfocusedonextendinggraph
theory into temporal graphs. A plethora of approaches, 3.1 Notations and Problem Formulation For-
including randomized reference models [21], stochastic mally, for different time intervals 0,1,··· ,T, a dynamic
block models [6] and models based on temporal random graphG:={G 0,G 1,··· ,G T}isdescribedbyitstopolog-
walk [22] to name a few, have been proposed with icalandattributedinformation,whereforeachtimestep
applicationsinnumerousdomains. Suchmethodsusually t∈T we have the graph snapshot G t =(E t,F t) where
Copyright©2021bySIAM
739 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
. 511.042.081.041
ot
42/41/20
dedaolnwoD
Figure 1: Graphic(aal) iGlleunsetrartaotrion of the proposed(mb)o Fdaecltosr.iz(ead) ETnchodeeBr ayesian network(c)o Ffutll hEencpordoepr osed probabilistic
distribution of dynamic graphs. (b) The approximate inference model of the posterior of the proposed model,
with conditional independence assumption among latent variables across different time points. (c) The alternative
approximate inference model of the posterior of the proposed model, with dependence among time-variant and
invariant variables and dependence across different time points.
the adjacency matrix is denoted as E ∈{0,1}n×n and dynamics that simultaneously involve both nodes and
t
node feature vector is represented as F ∈ Rn×c. Here edges such as the process of homophily and influence
t
n is the maximum number of nodes and each node has in social networks. Here we leverage the variable z(cid:48)(cid:48) to
c features. If there is a connection between nodes i account for it. In the following, we use the subscript t
and j then the element [E ] =1; otherwise, we have to denote any variable specific to the time t. In all, the
t i,j
[E ] = 0. Note that we sometimes use the notations generative process can be introduced as follows: t i,j
E = {E }T and F = {F }T for simplicity. In this
t t=0 t t=0
paper, we focus on factorized deep generative models for (3.1) p(E,F|z,z(cid:48),z(cid:48)(cid:48),f)=p(E|z,z(cid:48)(cid:48),f)p(F|z(cid:48),z(cid:48)(cid:48),f)
dynamic graphs that can learn the distribution p(G|Θ) (cid:89)
of dynamic graphs G generated from latent semantic = p(E t|z t,z t(cid:48)(cid:48),f)·p(F t|z t(cid:48),z t(cid:48)(cid:48),f)
t
variables Θ={f,z,z(cid:48),z(cid:48)(cid:48)} that characterize the genera-
tiveprocessfortime-invariantgraphpatterns(byf)and 3.3 Variational Model Inference Since the true
time-variantgraphpatterns,whichcanbefurtherfactor- posterior p(z,z(cid:48),z(cid:48)(cid:48),f|E,F) of the proposed generative
ized into edge-exclusive patterns (by z), node-exclusive model is intractable to infer, we proposed to solve
patterns (by z(cid:48)), and node-edge joint patterns (by z(cid:48)(cid:48)). it based on variational inference where the posterior
needs to be approximated by another distribution
3.2 Factorized Bayesian Models of Dynamic
q(z,z(cid:48),z(cid:48)(cid:48),f|E,F). So the goal is to minimize the
Graphs To achieve the above goal, we formulate the Kullback–Leibler (KL) divergence between the true and
proposed generative model as a Bayesian network, as approximate posteriors. Here, we will first introduce the
shown in Figure 1(a). Here as mentioned above, a approximate posteriors and then introduce the objective
dynamic graph G is a sequence of snapshots of edge- functions for model inference based on them.
T
node pairs {(E ,F ),(E ,F ),··· ,(E ,F )}, which are Inference models. We propose two different inference
1 1 2 2 T T
collectively controlled by a time-independent latent models based on two different assumptions on the
variable f to absorb the time-invariant patterns related conditional independence among variables, namely a
to the intrinsic nature of the graphs that do not change factorized inference model and a full inference model.
over time. To characterize the time-variant patterns, Factorized inference model. As shown in Figure 1(b),
additional latent variables z, z(cid:48), and z(cid:48)(cid:48) are introduced. the inference model can be formulated as a factoriza-
Here z is to model the dynamics only related to edges. tion for the latent variables with mutual conditional
For example, in dynamic functional connectivity of a independence, as below:
human brain, the co-activation patterns among different
brainregionscanchangequicklyovertime. z(cid:48)istomodel q(z,z(cid:48),z(cid:48)(cid:48),f|E,F)=q(z|E)q(z(cid:48)|F)q(z(cid:48)(cid:48)|E,F)q(f|E,F)
the dynamics merely pertain to nodes. For example, in
trafficnetworkwherenodesareroadsegmentsandedges where we assume the conditional independence among
are their connections, only the node attributes such all the variables z,z(cid:48),z(cid:48)(cid:48) and F.
as traffic flow changes but their connections (i.e., road Herewefurtherdecomposethemodelalongthetem-
network) are unchanged. In addition, there are also poraldimension,byassumingthetemporallyconditional
independence among the variable across different time
Copyright©2021bySIAM
740 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
. 511.042.081.041
ot
42/41/20
dedaolnwoD
points, by the following equations: Alternatively, for the full inference model, we have:
(3.2) q(z|E)=(cid:89) tq(z t|E t), q(z(cid:48)|F)=(cid:89) tq(z t(cid:48)|F t), m θ,a φxE Θ∼qφ(Θ|E,F)(cid:2) log(p θ(E|z,z(cid:48)(cid:48),f)p θ(F|z,z(cid:48)(cid:48),f))(cid:3) −
q(z(cid:48)(cid:48)|E,F)=(cid:89) tq(z t(cid:48)(cid:48)|E t,F t) (cid:88) t(cid:2) KL(q φ(z t|z <t,E t,f)||p(z t))+KL(q φ(z t(cid:48)|z <(cid:48) t,F t,f)||p(z t(cid:48)))
+KL(q (z(cid:48)(cid:48)|z(cid:48)(cid:48) ,E ,F ,f)||p(z(cid:48)(cid:48)))+KL(q (f|E ,F )||p(f))(cid:3)
where the latent variables for different time points are φ t <t t t t φ t t
conditionally independent from each other given the
3.4 Model Architecture The model inference ob-
observations for the corresponding time points.
jectives proposed in the previous section are equivalent
Full inference model. We could also weaken our assump- to learning the parameters θ and φ for the distributions
tion of conditional independence among time-variant
of generator and inference models. However, due to the
and time-invariant variables, by an alternative inference
extremely complicated distribution patterns for graph
model shown in Figure 1(c). Specifically, we have:
structured data, it is prohibitively difficult to predefine
any prescribed simple distribution and hence a highly
(3.3) q(z,z(cid:48),z(cid:48)(cid:48),f|E,F)
expressive model that can flexibly learn the (unknown)
=q(z|E,f)·q(z(cid:48)|F,f)·q(z(cid:48)(cid:48)|E,F,f)·q(f|E,F)
distribution is preferred. Inspired by the recent suc-
where we can see by assuming z,z(cid:48) and z(cid:48)(cid:48) to depend on cess in learning complicated distributions for complex
f, we allow that the time-variant patterns of graphs are data such as images and texts, we leverage graph deep
dependent on intrinsic static properties of the graphs. learning models to fit the underlying distributions of
Wefurtherdecomposethemodelalongthetemporal generatorandinferencemodels,parameterizedbyneural
dimension, by allowing temporal dependence among the network parameters θ and φ, respectively.
variables of different time points as follows: Architecture of the generator: First, for edge
generation,weuseadenselayerwiththeinputsincluding
(cid:89) (cid:89)
q(z|E)= q(z |z ,E ), q(z(cid:48)|F)= q(z(cid:48)|z(cid:48) ,F ), z,z(cid:48)(cid:48) and f. Then, we utilize a graph deconvolutional
t <t t t <t t
t t network [25] to generate a matrix of edge probabilities
(cid:89)
(3.4) q(z(cid:48)(cid:48)|E,F)= q(z t(cid:48)(cid:48)|z <(cid:48)(cid:48) t,E t,F t) on each time snapshot. For node generation, we use
t
another dense layer which takes the input that is the
where the latent variables for different time points are
concatenation of z(cid:48),z(cid:48)(cid:48), and f, and output each node
dependent on their previous time points as well as the
features on each time snapshot.
observations for the corresponding time points.
Architecture of full inference model. First, we
ObjectivefunctionsGiventheabove-definedinference
utilize a Graph Convolutional Network [32] to produce
models, the inference of the posterior of the proposed
generativemodelsinEquation(3.1)requirestominimize a representation vector a t for each snapshot graph:
the KL(q(z,z(cid:48),z(cid:48)(cid:48),f|E,F)||p(z,z(cid:48),z(cid:48)(cid:48),f|E,F)), where a
t
=GCN(E t). Raw node attributes are also converted
KL(·) denotes KL divergence. It is equivalent to toafeaturevectorthroughadenselayerb t =Dense(F t)
maximizing the following evidence lower bound (ELBO) first. To encode f, q (f|E,F) is modeled with a
φ
of our model: neural network function h (E,F) (cid:55)→ f. In details, we
φ
maxE (cid:2) log(p (E,F|Θ))(cid:3) −KL(q (Θ|E,F)||p(Θ)) concatenate latent representation of topology a t and
θ,φ Θ∼qφ(Θ|E,F) θ φ attribute representation b and feed into a Bi-directional
t
where Θ = {z,z(cid:48),z(cid:48)(cid:48),f} is the set of all the variables LSTM BiLSTM as inputs. Then, both the last forward
while θ and φ explicitly denote the parameters of output m T and the first backward output m¯ T are
the distributions corresponding to our generator and concatenated to a single vector. This vector is passed to
inference model, respectively. The prior distribution anotherdenselayertogetf. Thefunctionh (E,F)(cid:55)→f
φ
p(Θ) = p(z,z(cid:48),z(cid:48)(cid:48),f) = p(z)p(z(cid:48))p(z(cid:48)(cid:48))p(f), where is decomposed as follow:
each variable follows an isotropic Gaussian distribution.
Recallthatwehaveproposedtwoinferencemodelswhich (3.5) a =GCN(E ),b =Dense(F )
t t t t
arethetwopossibleoptionsforinstantiatingq (Θ|E,F),
φ m ,m¯ =BiLSTM(a ||b ), f =Dense(m ||m¯ )
namely factorized inference model and full inference t t t t T T
model. When employing different inference model we
where || is the concatenation operation. With similar
have the corresponding objective function. First, for
way, we leverage another three Bi-directional LSTMs to
factorized inference model, we have:
encode z,z(cid:48) and z(cid:48)(cid:48). For the details of them, please refer
maxE (cid:2) log(p (E|z,z(cid:48)(cid:48),f)p (F|z(cid:48),z(cid:48)(cid:48),f))(cid:3) − to our supplementary material.
θ,φ
Θ∼qφ(Θ|E,F) θ θ
Architecture of factorized inference model. The
(cid:88) (cid:2) KL(q φ(z t|E t)||p(z t))+KL(q φ(z t(cid:48)|F t)||p(z t(cid:48)))+ encoding structure for f is the same as the above. The
t
KL(q φ(z t(cid:48)(cid:48)|E t,F t)||p(z t(cid:48)(cid:48)))+KL(q φ(f|E t,F t)||p(f))(cid:3) only difference focuses on the encoding of z t, z t(cid:48), and z t(cid:48)(cid:48).
Copyright©2021bySIAM
741 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
.
511.042.081.041
ot
42/41/20
dedaolnwoD
Different from the above full inference model, we simply 4.2 Experimental Setup We compare our method
usethreemulti-layerperceptionmodelstogeneratethem, with several state-of-the-art deep graph generative
using the inputs E , F , and jointly E ,F , respectively. modelsbasedonasetofmetricsmeasuringthesimilarity
t t t t
No bi-directional LSTM is needed here. betweenrealandgeneratedgraphsintopologyandnode
features.
3.5 Complexity Analysis The computational effi-
ciency is an essential prerequisite for large-scale graph Table 1: Comparison of D2G2 to baseline models using MMD
generation. Most of the existing graph generation meth- metricswiththebestperformancemarkedinboldface.
ods require O(n3), e.g., GraphVAE, or even O(n4), e.g., Nodes’
Li et al., time in general, thus limiting their applications Dataset MeM the ot drics B C itye et nw tre ae ln -nessB C ityr eo na trd ac la -st B C ityu enrs tt ri an l-ess T p Coe om r ra r- l
ela-
R C ityee nce ti rv ae l- T C tie oom nrrp elo ar -al
in generating small graphs. The overall time complexity tion
of the proposed D2G2 is O(cTn2) and is therefore capa- Protein G N Ger ra atGp ph hA VR N AN EN 00 0.. .98 831 006 350 6 000 ... 020 292 902 574 91 8.. .42 235 0ee e-- -00 054 5 010 ... 969 211 256 016 0 00 ... 020 121 442 546 00 0 .. . 00 3 88 6 22 6 12 3
ble of generating modest scale attributed graphs with DSBM 0.4169 0.5787 1.06e-05 1.5632 0.7479 0.4446
hundreds or thousands of nodes. D D2 2G G2 2_full 0 0. .3 30 04 42 2 0 0. .0 00 09 99 8 66 .. 77 55 ee -- 00 77 00 .. 00 00 117 7 0 0. .0 01 17 74 3 1 1. .1 19 9e e- -0 04 4
GraphRNN 0.2281 0.0390 0.0160 0.2899 0.1128 5.35e-04
Auth. N Ge ratG phA VN AE 0 0. .1 24 01 88 4 00 .. 16 05 44 08 0 0. .0 01 18 44 1 00 .. 34 28 17 64 00 .. 06 97 71 94 90. .0 93 17 e6 -04
4 Experiments DSBM 0.3255 0.0020 0.0035 0.2697 0.0053 0.0052
D2G2 0.0017 0.0379 2.86e-05 0.2339 0.0365 0.0549
In this section, the performance of our model is eval- D2G2_full 0.0017 0.0379 2.86e-05 0.2339 0.0365 0.0549
GraphRNN NaN 0.0526 1.28e-04 0.6854 5.93e-05 0.0023
uated using several synthetic and real-world datasets Metro N Ge ratG phA VN AE 1 N. a0 N 04 .. 052 7e 4- 00 54 0 6. .0 01 04 e-6 05 01 .. 69 76 45 25 2 0. .0 07 2e 8- 204 0 3. .5 74 32 e5 -04
against the state-of-the-art, on various aspects including D DS 2B GM 2 0N .a 0N 878 4 1.. 54 93 ee -- 00 67 6 1. .5 44 6e e- -0 06 6 0 0. .3 10 18 88 9 4 3.. 56 00 ee -- 00 67 0 0. .0 00 03 10 8
quantitative, qualitative and efficiency analyses. The D Gr2 aG ph2 R_ Nfu Nll 00 .. 90 58 67 79 1 0. .5 19 6e 5-0 86 1 0.. 34 76 9e 0-06 00 .. 01 01 18 19 3 0. .5 30 0e 2- 306 10 .. 80 10 e1 -8 06
experiments were performed on a 64-bit machine with 100 N Ge ratG phA VN AE 0 0. .6 94 59 67 7 00 .. 27 10 65 78 0 0. .0 40 19 32 8 00 .. 00 00 11 14 00 .. 32 58 37 98 17 .. 83 11 ee -- 00 66
DSBM 0.0020 0.4016 0.0526 0.0183 0.1317 1.33e-04 a 10-core processor (i9, 3.3GHz), 64GB memory with D2G2 0.0204 0.1874 1.98e-05 9.12e-05 0.2357 1.69e-06
D2G2_full 0.0203 0.1873 1.98e-05 9.12e-05 0.2358 1.69e-06
GTX 1080Ti GPU. GraphRNN 0.7912 0.1556 0.1621 0.0049 0.4241 1.75e-07
500 NetGAN 0.7928 0.3253 0.0415 0.0049 0.0948 1.75e-07
GraphVAE 0.7928 0.0871 0.1921 0.0049 0.2858 1.75e-07
D2G2 0.0015 0.0124 4.19e-05 6.09e-04 0.1904 7.85e-07
4.1 Datasets We evaluate our proposed D2G2 on D2G2_full 0.0015 0.0124 4.19e-05 6.09e-04 0.1904 7.85e-07
both real and synthetic benchmark datasets with graph 2500 G Nera tGph AR NNN 00 .. 88 88 00 12 01 .. 10 22 03 09 09 .. 06 15 6e 9-07 0 0. .0 00 04 44 4 1 0. .2 04 91 60 5 1 1. .0 00 0e e- -0 08 8
D2G2 0.0002 0.0056 7.25e-05 0.0005 0.1487 8.18e-05
sizesnrangingfrom8to2500anddiversecharacteristics. D2G2_full 0.0002 0.0056 7.25e-05 0.0005 0.1487 8.18e-05
Protein. The dynamic folding process of a protein Table2: ComparisonofD2G2tobaselinemodelsbasedonnode
peptide is simulated with a sequence AGAAAAGA. For
attributegenerationwiththebestperformancemarkedinboldface.
graph learning, this can be considered as a graph of
8 nodes with node attributes (x,y,z) corresponding to Dataset MM ee tt hr oic ds MSE R2 PCC
3D coordination of the C α atom of each amino acid, Protein G Drap 2h GVA 2E 00 .. 00 008 40 0 00 .. 983 7 0 0. .7 92 7
producing 300 temporal graphs with a sequence length
Auth.
D G2 rG ap2 h_ VAfu Ell 0 0. .0 00 034 60 00 .. 99 56 0 0. .9 878
of 100. D2D G2 2G _2 full 00 .. 00 00 224 4 00 .. 99 99 0 0. .9 93 3
Authentication. 97 users’ authentication Metro G Drap 2h GVA 2E 0 0. .0 00 00 06 4 00 .. 992 5 0 0. .8 82 6
D2G2_full 0.0003 0.95 0.87
graphs [33]. Each user’s graphs are generated by au- 100 G Drap 2h GVA 2E 12 .. 913
7
0 0. .8 766 00 .. 989
1
thentication activities on their accessible 27 computers D2G2_full 1.97 0.76 0.91
or servers, i.e., n = 27, in an enterprise computer net- 500 DG 2Dr Gap 2 2h G _VA 2 fuE
ll
44 4 25 3 .. .9 7 70 7
7
0 00 . .. 5 54 15 2 0 00 . .. 4 439 7
7
work during a 485h period. All times are normalized in 2500 G Drap 2h GVA 2E 1 15 .. 45 96 00 .. 875
9
00 .. 878
1
the range of [0, 1]. D2G2_full 1.48 0.89 0.81
Metro. Metro graph data captured by farecard Comparison Methods. 1) GraphVAE [10] is the
records from the Washington D.C. metro system which pioneering variational autoencoder based graph genera-
reflects million of users’ trips records from May 2016 to tionmethod;2)NetGAN[13]trainsthegraphgenerative
July 2016. There are 91 stations as graph nodes and model with the GAN algorithm; 3) GraphRNN [27] is
each day is treated as a temporal graph sample. the recent generative model for graphs based on sequen-
Synthetic datasets. We also consider three tial node and edge generation; 4) Dynamic-Stochastic-
syntheticdatasetswithincreasingcomplexityfromscale- Blocks-Model(DSBM)[35],whichextendsthestochastic
free random graphs [34]. Existing works use it as static blockmodel for static networks to the dynamic setting.
graph, we append a continuous-time value to generated Among them, GraphVAE is the only existing method
edge in each constructing step to simulate it as dynamic that achieves graph disentanglement, i.e., learning the
graphs, resulting 3 synthetic datasets with number of topology of graphs but also node features. 5) Disen-
nodes n=100,500,2500, respectively. tangled Dynamic Graph Deep Generation (D2G2), our
proposedmodels(factorizedandfull)inthispaperwhose
Copyright©2021bySIAM
742 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
. 511.042.081.041
ot
42/41/20
dedaolnwoD
(a)Groundtruth (b)D2G2 (c)dsbm
(d)graphrnn (e)graphvae (f)netgan
Figure2: VisualizationofgraphsfromProteindatasetandgeneratedbyallmethods. Thecolorofnodesrepresents
the normalized value of a node attribute.
parameter setting is elaborated in the supplementary only generate smaller graphs, D2G2 enjoys the merit of
material. handling graphs with increasing complexity. In terms of
Evaluation metrics. Various maximum mean dis- node attributes, GraphVAE is the only method capable
crepancy(MMD)metrics[36],includingbetweennesscen- of handling node attribute prediction, the proposed
trality, broadcast centrality, burstiness centrality, node D2G2 outperforms GraphVAE significantly and can
temporal correlation, receive centrality and temporal be as high as 88% on related statistics while scaling
correlation are utilized to evaluate the performance of to thousand level graphs where GraphVAE fails. The
our models and other baselines in terms of topology superior performance in both graph topology and node
simulation. The lower these MMD values the merrier a attributes generation of D2G2 verifies the necessity of
generative model. Additional measures including mean its theoretically disentangling design, which captures
squared error (MSE), coefficiency of determination score the underlying graph dynamics very well. Note that in
(R2) and Pearson correlation coefficient (PCC) are also the conducted experiments, factorized and full encoder
evaluated for node attribute evaluation to shed full light networks produce almost identical results, presumably
on the similarity between the real target and generated because respective factors are truly independent. We
graphs. therefore only report results on factorized network in
the following discussions.
4.3 Generating Attributed Temporal Graphs Temporal graph visualization. Wevisualizethe
Wepresentbothquantitativeandqualitativeexperiment graphs generated by D2G2 and various baselines to
results that demonstrate the effectiveness and efficiency qualitatively evaluate their dynamic graph generation
of D2G2 in generating attributed temporal graphs. capabilities. In the visualization, nodes of the temporal
Evaluation with graph statistics. A set of graphs span all temporal snapshots, while arcs are edges
topology related MMD metrics and node attribute reflectingconnectivitiesamongthenodesofthetemporal
evaluationsaretestedondifferentmodelsandtheresults snapshots. Nodeattributesarerepresentedbythecolors
are summarized in Tables 1 and 2, respectively. (white ones denote those who are incapable of learning
As one can see, D2G2 based models consistently node features) of nodes. We use the protein dataset as
achieve superior performance on most of metrics across the running example and more qualitative evaluations
all datasets in both node and attribute generation, and are available in the supplementary material. Figure 2
are the only methods to handle both properly. On the showsthegraphsgeneratedfortheproteindatasetbyall
other hand, although some baseline models perform themethods. ItclearthatD2G2issuperiorincapturing
well on certain specific datasets, they cannot do well the underlying characteristics of graphs and is closest to
for other types of input graphs. In addition, while the ground truth in Figure 2(a), in terms of both edges
methods such as DSBM does not scale well and can and nodes. GraphVAE is the second best one whose
Copyright©2021bySIAM
743 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
.
511.042.081.041
ot
42/41/20
dedaolnwoD
(a)Generatedgraphswhenvaryingf withz,z(cid:48) andz(cid:48)(cid:48) fixed.
(b)Generatedgraphswhenvaryingz withf,z(cid:48) andz(cid:48)(cid:48) fixed.
(c)Generatedgraphswhenvaryingz(cid:48) withf,z andz(cid:48)(cid:48) fixed.
(d)Generatedgraphswhenvaryingz(cid:48)(cid:48) withf,z andz(cid:48) fixed.
Figure 3: Visualization of Protein graphs generation controlled by respective disentangling factors.
edge weight generation are worse than D2G2. tion is indeed factorized and can control the correspond-
Evaluationofthegenerateddisentangledpat- ing patterns related to them. For example, as shown
terns. We further qualitatively evaluate whether our in Figure 3(a), when we fix z, z(cid:48) and z(cid:48)(cid:48) while vary f,
D2G2 indeed disentangles dynamic and static patterns the whole dynamic graph changes evenly across different
in nodes and edges, by randomly sampling one of f, z, time snapshots, which verifies that f indeed controls the
z(cid:48) and z(cid:48)(cid:48) while keeping the other three fixed. This can invariant patterns of the dynamic graphs. Differently,
control the respective factors of static patterns, edge whenweinsteadvaryz byfixingz(cid:48),z(cid:48)(cid:48),andf,thetopol-
dynamics, node dynamics as well as edge-node joint ogy in different time snapshots for each graph varies,
dynamics, respectively. We have observed numerous which demonstrate that z indeed effectively controls the
interesting patterns among different datasets and here non-stationary patterns in each dynamic graph; we can
list some of them as shown in Figure 3, while more are also see z(cid:48) is capable of finding time variant factor con-
in the supplementary material. trolling attribute as the colors (i.e., the corresponding
Theresultsdemonstratethatthelearnedrepresenta- node attributes values they reflect) vary instead of the
Copyright©2021bySIAM
744 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
.
511.042.081.041
ot
42/41/20
dedaolnwoD
[3] G. Robins and P. Pattison, “Random graph models [20] D. Borsboom, “A network theory of mental disorders,”
for temporal processes in social networks,” Journal of World psychiatry, vol. 16, no. 1, pp. 5–13, 2017.
Mathematical Sociology, vol. 25, no. 1, pp. 5–41, 2001. [21] M. Karsai, M. Kivelä, R. K. Pan, K. Kaski, J. Kertész,
[4] S. G. Aksoy, E. Purvine, E. Cotilla-Sanchez, and A.-L. Barabási, and J. Saramäki, “Small but slow
M. Halappanavar, “A generative graph model for world: Hownetworktopologyandburstinessslowdown
electrical infrastructure networks,” Journal of Complex spreading,” Physical Review E, vol. 83, no. 2, 2011.
Networks, vol. 7, no. 1, pp. 128–162, 2019. [22] M. Starnini, A. Baronchelli, A. Barrat, and R. Pastor-
[5] M. Alam, K. S. Perumalla, and P. Sanders, “Novel Satorras, “Random walks on temporal networks,” Phys-
parallel algorithms for fast multi-gpu-based generation ical Review E, vol. 85, no. 5, p. 056115, 2012.
of massive scale-free networks,” Data Science and [23] S. Xiao, M. Farajtabar, X. Ye, J. Yan, L. Song, and
Engineering, vol. 4, no. 1, pp. 61–75, 2019. H. Zha, “Wasserstein learning of deep generative point
[6] T. Louail, M. Lenormand, M. Picornell, O. G. Cantú, process models,” in Neurips, 2017, pp. 3247–3257.
R. Herranz, E. Frias-Martinez, J. J. Ramasco, and [24] K. Xu, L. Wu, Z. Wang, Y. Feng, M. Witbrock, and
M. Barthelemy, “Uncovering the spatial structure of V. Sheinin, “Graph2seq: Graph to sequence learning
mobility networks,” Nature communications, vol. 6, with attention-based neural networks,” arXiv preprint
no. 1, pp. 1–8, 2015. arXiv:1804.00823, 2018.
[7] A. Radford, L. Metz, and S. Chintala, “Unsu- [25] X. Guo, L. Wu, and L. Zhao, “Deep graph translation,”
pervised representation learning with deep convolu- arXiv preprint arXiv:1805.09980, 2018.
tional generative adversarial networks,” arXiv preprint [26] D. P. Kingma and M. Welling, “Auto-encoding varia-
arXiv:1511.06434, 2015. tional bayes,” arXiv preprint arXiv:1312.6114, 2013.
[8] Z. Xie, “Neural text generation: A practical guide,” [27] J. You, R. Ying, X. Ren, W. L. Hamilton, and
arXiv preprint arXiv:1711.09534, 2017. J. Leskovec, “Graphrnn: Generating realistic graphs
[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, with deep auto-regressive models,” arXiv preprint
D.Warde-Farley,S.Ozair,A.Courville,andY.Bengio, arXiv:1802.08773, 2018.
“Generative adversarial nets,” in Advances in neural [28] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Mur-
information processing systems, 2014, pp. 2672–2680. phy, “Deep variational information bottleneck,” arXiv
[10] M. Simonovsky and N. Komodakis, “Graphvae: To- preprint arXiv:1612.00410, 2016.
wards generation of small graphs using variational au- [29] H.Dai,Y.Tian,B.Dai,S.Skiena,andL.Song,“Syntax-
toencoders,” in International Conference on Artificial directed variational autoencoder for structured data,”
Neural Networks. Springer, 2018, pp. 412–422. arXiv preprint arXiv:1802.08786, 2018.
[11] M. AlQuraishi, “Alphafold at casp13,” Bioinformatics, [30] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud,
vol. 35, no. 22, pp. 4862–4865, 2019. “Isolating sources of disentanglement in variational
[12] W.-N. Hsu and J. Glass, “Scalable factorized hierarchi- autoencoders,” in Neurips, 2018, pp. 2610–2620.
cal variational autoencoder training,” arXiv preprint [31] W.-N.Hsu,Y.Zhang,andJ.Glass,“Unsupervisedlearn-
arXiv:1804.03201, 2018. ing of disentangled and interpretable representations
[13] A. Bojchevski, O. Shchur, D. Zügner, and S. Günne- from sequential data,” in Neurips, 2017, pp. 1878–1889.
mann, “Netgan: Generating graphs via random walks,” [32] T. N. Kipf and M. Welling, “Semi-supervised classifica-
arXiv preprint arXiv:1803.00816, 2018. tionwithgraphconvolutionalnetworks,” arXivpreprint
[14] G. H. Nguyen, J. B. Lee, R. A. Rossi, N. K. Ahmed, arXiv:1609.02907, 2016.
E.Koh,andS.Kim,“Continuous-timedynamicnetwork [33] A. D. Kent, “Cyber security data sources for dynamic
embeddings,” intheWebConference,2018,pp.969–976. network research,” in Dynamic Networks and Cyber-
[15] S. Bhagat, G. Cormode, and S. Muthukrishnan, “Node Security. World Scientific, 2016, pp. 37–65.
classificationinsocialnetworks,” inSocial network data [34] A.-L. Barabási, R. Albert, and H. Jeong, “Mean-field
analytics. Springer, 2011, pp. 115–148. theory for scale-free random networks,” Physica A:
[16] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An Statistical Mechanics and its Applications, vol. 272, no.
end-to-end deep learning architecture for graph classifi- 1-2, pp. 173–187, 1999.
cation,” in AAAI, 2018. [35] K. S. Xu and A. O. Hero, “Dynamic stochastic block-
[17] D. Liben-Nowell and J. Kleinberg, “The link-prediction models for time-evolving social networks,” IEEE Jour-
problem for social networks,” JASIST, vol. 58, no. 7, nal of Selected Topics in Signal Processing,vol.8,no.4,
pp. 1019–1031, 2007. pp. 552–562, 2014.
[18] M. Zhang, X. Zhao, W. Zhang, A. Chaddad, A. Evans, [36] A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf,
and J. B. Poline, “Deep discriminative learning for and A. J. Smola, “A kernel method for the two-sample-
autism spectrum disorder classification,” in DEXA. problem,” in Neurips, 2007, pp. 513–520.
Springer, 2020, pp. 435–443. [37] W. Zhang and E. Ntoutsi, “Faht: an adaptive fairness-
[19] S. Jiang, J. Ferreira, and M. C. Gonzalez, “Activity- awaredecisiontreeclassifier,” inIJCAI,2019,pp.1480–
based human mobility patterns inferred from mobile 1486.
phone data: A case study of singapore,” IEEE Trans- [38] W. Zhang and L. Zhao, “Online decision trees with
actions on Big Data, vol. 3, no. 2, pp. 208–219, 2017. fairness,” arXiv preprint arXiv:2010.08146, 2020.
Copyright©2021bySIAM
746 Unauthorizedreproductionofthisarticleisprohibited
ycavirp-smret/gro.mais.sbupe//:sptth
ees
;thgirypoc
ro
esnecil
MAIS
ot
tcejbus
noitubirtsideR
.
511.042.081.041
ot
42/41/20
dedaolnwoD

